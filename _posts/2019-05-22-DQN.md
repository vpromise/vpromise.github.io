---
bg: "photo07.jpg"
layout: post
title: "DQN"
crawlertitle: DQN #"page title"
summary: DQN #"post description"
date: 2019-05-22 22:00:00 +0800
tags : 'DL DQN'
author: "vpromise"
categories: "paper"
---


*分享最近读的论文，将深度学习与强化学习相结合的Deep Reinforcement Learning*

*由于Github Pages无法显示markdown公式，对于编辑造成了很大麻烦，考虑是否继续写本文*

---
先放论文

[Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)

[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

这两篇都是Google DeepMind提出的将深度学习与强化学习相结合，用AI来玩游戏，并取得了超过人类玩家的水平，下面我们来看论文

## Reinforcement Learning
*先介绍一下关于强化学习的知识*
维基百科是这样介绍的：Reinforcement learning (RL)is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. .<https://en.wikipedia.org/wiki/Reinforcement_learning>
好了，让我们先了解一些概念：
* **agent**  代理，即智能体，RL中的主体，游戏中对应玩家
* **environment**  指代理所处的环境，如游戏中的地图
* **action**  agent所采取的动作，如游戏中的上下左右移动
* **state**  agent的当前状态，如位于地图中某个特定位置
* **reward**  environment给予agent的奖励，可以正负，如游戏中杀死boss获得正分
![RL framework](https://i.loli.net/2019/05/23/5ce6081b2241930320.png)<center>Fig.1.RL framework</center>

如上图所示，展现了RL的一个step `t`，我们的**agent**从**environment**获得目前所处的**state** s<sub>t</sub> 和**reward** r<sub>t</sub>,从而作出**action** a<sub>t</sub>，而这个a<sub>t</sub>又会作用于我们的**environment**，从而改变**state** s<sub>t</sub>，并作出**reward** r<sub>t</sub>，就在这样的一个机制中，我们的**agent**从**reward**中学习采取最优的**action**，从而取得最大化的预期利益。

## Q-Learning
Q-Learning算法介绍

### Introduction

**强化学习理论提供了一个规范的深植于动物行为心理学和神经科学视角的说明，解释了代理(agent)如何优化他们对环境的控制。然而，为了在现实世界中复杂的情况下成功地使用强化学习，代理面临着一项艰巨的任务：他们必须从高维度输入中获得环境的有效表示，并使用这些信息来将过去的经验推广到新的情境中。值得注意的是，人类和其他动物似乎通过强化学习和分层感觉处理系统的和谐组合来解决这个问题，前者通过丰富的神经数据证明了多巴胺能神经元和时间差异强化学习算法发出的相位信号之间的显着相似性。虽然强化学习代理已经在各种领域中取得了一些成功，但它们仅适用于可以手工制作有用特征的领域，或者具有完全观察到的低维状态空间的领域。本文中，我们利用最新的深度神经网络训练来开发一种新的人工智能体，称为deep Q-network(DQN)，可以使用端到端强化学习方法直接从高维感知输入中学习成功的策略。我们在经典的Atari 2600游戏的挑战性领域测试了这个代理。我们证明了，仅接收像素和游戏分数作为输入的DQN网络代理能够超越所有先前算法的性能，在49个游戏中达到与专业人类游戏测试者相当的水平，并且使用的是相同的算法，网络架构和超参数。这项工作弥合了高维度感官输入和行动之间的鸿沟，从而产生了第一个能够学习在各种具有挑战性的任务中表现出色的人工智能代理。**

我们开始创建一个单一的算法，能够在各种具有挑战性的任务中发展广泛的能力——一个先前的努力都没能实现的人工智能的核心目标。为此，我们开发了全新的代理，一个deep Q-network(DQN)，它能够将强化学习与人工神经网络即深度神经网络结合起来。值得注意的是，在深度神经网络的最新进展中，通过几层节点来逐步构建获取更抽象的数据表示，使得人工神经网络可以直接从原始传感数据中学习对象类别等概念。我们使用一个特别成功的架构，即深度卷积网络，它使用层叠的平铺卷积滤波器层来模拟感受域的影响——受Hubel和Wiesel在早期视觉皮层中的前馈处理的开创性工作的启发——从而利用当前的局部空间相关性图像，并建立稳健的自然变换，如视点或规模的变化。

我们考虑代理通过一系列观察，行动和奖励与环境交互的任务，代理的目标是以最大化未来累积奖励的方式选择行动。更正式地，我们使用深度卷积神经网络来近似最优动作值函数
$$ Q^*(s,a)=\max_\pi E[r_t+\gamma r{t+1}+ \gamma^2 r{t+2}+\cdots|s_t=s,a_t=a,\pi] $$
![fun.1](https://i.loli.net/2019/05/24/5ce75a1e44bc091605.png)
这是在每个时间步t由$\gamma$打折的最大奖励总和$r_t$，可以通过行为政策$\pi=P(a|s)$在进行观察(s)和采取行动(a)后实现（见方法）。


*未完待续……*
